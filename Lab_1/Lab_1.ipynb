{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "06376fb4-5094-4c07-8b75-44e928f58907",
      "cell_type": "code",
      "source": "import warnings\n\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport inspect\n\nEPS = 1e-3\n\n\ndef my_mse(y_true, y_pred):\n    return np.mean((np.array(y_true) - np.array(y_pred)) ** 2)\n\n\ndef my_mae(y_true, y_pred):\n    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n\n\ndef my_rmse(y_true, y_pred):\n    return np.sqrt(my_mse(y_true, y_pred))\n\n\ndef my_r2(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    return 1 - (ss_res / (ss_tot + 1e-8))\n\n\ndef my_mape(y_true, y_pred):\n    y_true, y_pred = np.array(y_true, dtype=np.float64), np.array(y_pred, dtype=np.float64)\n    mask = np.abs(y_true) > EPS\n    if np.sum(mask) == 0:\n        return 0\n    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / (np.abs(y_true[mask]) + EPS))) * 100\n\n\nclass MyLinearRegression:\n    def __init__(self, method='analytical', lr=0.01, epochs=1000, batch_size=32, verbose=False):\n        self.method = method\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.weights = None\n        self.bias = None\n        self.verbose = verbose\n        self.loss_history = []\n\n    def fit(self, X, y):\n        X = np.array(X, dtype=np.float64)\n        y = np.array(y, dtype=np.float64)\n        n_samples, n_features = X.shape\n\n        if self.method == 'analytical':\n            X_b = np.c_[np.ones((n_samples, 1)), X]\n            try:\n                theta = np.linalg.pinv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n            except:\n                theta = np.linalg.lstsq(X_b, y, rcond=None)[0]\n            self.bias = theta[0]\n            self.weights = theta[1:]\n\n        elif self.method == 'gd':\n            self.weights = np.zeros(n_features)\n            self.bias = 0\n            for epoch in range(self.epochs):\n                y_pred = np.dot(X, self.weights) + self.bias\n                error = y_pred - y\n                dw = (2 / n_samples) * np.dot(X.T, error)\n                db = (2 / n_samples) * np.sum(error)\n                self.weights -= self.lr * dw\n                self.bias -= self.lr * db\n                loss = np.mean(error ** 2)\n                self.loss_history.append(loss)\n\n        elif self.method == 'sgd':\n            self.weights = np.zeros(n_features)\n            self.bias = 0\n            for epoch in range(self.epochs):\n                indices = np.arange(n_samples)\n                np.random.shuffle(indices)\n                X_shuffled = X[indices]\n                y_shuffled = y[indices]\n                for i in range(0, n_samples, self.batch_size):\n                    X_batch = X_shuffled[i:i + self.batch_size]\n                    y_batch = y_shuffled[i:i + self.batch_size]\n                    y_pred = np.dot(X_batch, self.weights) + self.bias\n                    error = y_pred - y_batch\n                    dw = (2 / len(X_batch)) * np.dot(X_batch.T, error)\n                    db = (2 / len(X_batch)) * np.sum(error)\n                    self.weights -= self.lr * dw\n                    self.bias -= self.lr * db\n                y_pred_epoch = np.dot(X, self.weights) + self.bias\n                loss = np.mean((y_pred_epoch - y) ** 2)\n                self.loss_history.append(loss)\n\n    def predict(self, X):\n        X = np.array(X, dtype=np.float64)\n        return np.dot(X, self.weights) + self.bias\n\n\ndef k_fold_cross_validation(X, y, k=5, method='analytical'):\n    n_samples = len(X)\n    fold_size = n_samples // k\n    scores_mse = []\n    for fold_idx in range(k):\n        start_idx = fold_idx * fold_size\n        end_idx = start_idx + fold_size if fold_idx < k - 1 else n_samples\n        X_val = X[start_idx:end_idx]\n        y_val = y[start_idx:end_idx]\n        X_train = np.vstack([X[:start_idx], X[end_idx:]])\n        y_train = np.hstack([y[:start_idx], y[end_idx:]])\n        model = MyLinearRegression(method=method)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_val)\n        mse = my_mse(y_val, y_pred)\n        scores_mse.append(mse)\n    return np.array(scores_mse)\n\n\ndef leave_one_out_cross_validation(X, y, method='analytical', max_samples=500):\n\n    n_samples = min(len(X), max_samples)\n    indices = np.random.choice(len(X), n_samples, replace=False)\n    X_sample = X[indices]\n\n    if isinstance(y, pd.Series):\n        y_sample = y.iloc[indices].reset_index(drop=True).values\n    else:\n        y_sample = y[indices]\n\n    errors_mse = []\n    errors_mae = []\n\n    for i in range(n_samples):\n        X_train = np.vstack([X_sample[:i], X_sample[i + 1:]])\n        y_train = np.hstack([y_sample[:i], y_sample[i + 1:]])\n        X_test = X_sample[i:i + 1]\n        y_test = y_sample[i]\n\n        model = MyLinearRegression(method=method)\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)[0]\n\n        error_mse = (y_test - y_pred) ** 2\n        error_mae = np.abs(y_test - y_pred)\n\n        errors_mse.append(error_mse)\n        errors_mae.append(error_mae)\n\n    return np.array(errors_mse), np.array(errors_mae)\n\n\n\ndef signed_log1p(data):\n    return np.sign(data) * np.log1p(np.abs(data))\n\n\ndef extract_date_parts(df, column):\n    if column not in df.columns:\n        return\n    parsed = pd.to_datetime(df[column], errors=\"coerce\")\n    df[f\"{column}_Year\"] = parsed.dt.year\n    df[f\"{column}_Month\"] = parsed.dt.month\n    df[f\"{column}_DayOfWeek\"] = parsed.dt.dayofweek\n    df[f\"{column}_Quarter\"] = parsed.dt.quarter\n    df.drop(columns=[column], inplace=True)\n\n\ndef add_bins(df, column, bins, fmt=\"num\"):\n    if column not in df.columns:\n        return\n    labels = []\n    for i in range(len(bins) - 1):\n        lo, hi = bins[i], bins[i + 1]\n        if np.isinf(hi):\n            labels.append(f\"{lo}{'+' if fmt == 'num' else ''}\")\n        else:\n            labels.append(f\"{lo}-{hi}\")\n    df[f\"{column}Band\"] = (pd.cut(df[column], bins=bins, labels=labels, include_lowest=True).astype(str))\n\n\ndef augment_features(df):\n    df[\"InterestRateSpread\"] = df[\"InterestRate\"] - df[\"BaseInterestRate\"]\n    df[\"LoanToIncome\"] = df[\"LoanAmount\"] / (df[\"AnnualIncome\"] + EPS)\n    total_debt = df[\"MonthlyLoanPayment\"] + df[\"MonthlyDebtPayments\"]\n    df[\"DebtServiceRatio\"] = total_debt / (df[\"MonthlyIncome\"] + EPS)\n    df[\"DisposableIncome\"] = df[\"MonthlyIncome\"] - total_debt\n    df[\"AssetCoverage\"] = df[\"TotalAssets\"] / (df[\"TotalLiabilities\"] + EPS)\n    df[\"LiabilityGap\"] = df[\"TotalLiabilities\"] - df[\"TotalAssets\"]\n    df[\"SignedLogLiabilityGap\"] = signed_log1p(df[\"LiabilityGap\"])\n    df.drop(columns=[\"LiabilityGap\"], inplace=True)\n    df[\"NetWorthToLiabilities\"] = df[\"NetWorth\"] / (df[\"TotalLiabilities\"] + EPS)\n    df[\"NetWorthToIncome\"] = df[\"NetWorth\"] / (df[\"AnnualIncome\"] + EPS)\n    df[\"UtilizationPerLine\"] = df[\"CreditCardUtilizationRate\"] / (df[\"NumberOfOpenCreditLines\"] + 1)\n    df[\"InquiryPerLine\"] = df[\"NumberOfCreditInquiries\"] / (df[\"NumberOfOpenCreditLines\"] + 1)\n    df[\"IncomePerDependent\"] = df[\"AnnualIncome\"] / (df[\"NumberOfDependents\"] + 1)\n    df[\"ExperienceToAge\"] = df[\"Experience\"] / (df[\"Age\"] + EPS)\n    df[\"LoanDurationYears\"] = df[\"LoanDuration\"] / 12.0\n    df[\"CreditHistoryToAge\"] = df[\"LengthOfCreditHistory\"] / (df[\"Age\"] + EPS)\n    df[\"IncomeDiscrepancy\"] = df[\"AnnualIncome\"] - (df[\"MonthlyIncome\"] * 12.0)\n    df[\"AgeAfterExperience\"] = df[\"Age\"] - df[\"Experience\"]\n\n    parsed = pd.to_datetime(df[\"ApplicationDate\"], errors=\"coerce\")\n    df[\"ApplicationDateWeek\"] = parsed.dt.isocalendar().week.astype(float)\n    df[\"ApplicationDateDayOfYear\"] = parsed.dt.dayofyear\n    df[\"ApplicationDateQuarter\"] = parsed.dt.quarter\n\n    df[\"CreditScore_2\"] = df[\"CreditScore\"] ** 2\n    df[\"CreditScore_3\"] = df[\"CreditScore\"] ** 3\n    df[\"Age_2\"] = df[\"Age\"] ** 2\n    df[\"Age_3\"] = df[\"Age\"] ** 3\n    df[\"SqrtAnnualIncome\"] = np.sqrt(np.abs(df[\"AnnualIncome\"]) + EPS)\n    df[\"SqrtLoanAmount\"] = np.sqrt(np.abs(df[\"LoanAmount\"]) + EPS)\n    df[\"SqrtMonthlyIncome\"] = np.sqrt(np.abs(df[\"MonthlyIncome\"]) + EPS)\n    df[\"LogCreditScore\"] = np.log1p(df[\"CreditScore\"])\n    df[\"LogExperience\"] = np.log1p(df[\"Experience\"])\n    df[\"LogAge\"] = np.log1p(df[\"Age\"])\n    df[\"LogCreditScore_2\"] = (np.log1p(df[\"CreditScore\"])) ** 2\n    df[\"LogAnnualIncome\"] = np.log1p(df[\"AnnualIncome\"] + EPS)\n    df[\"ExpNormCreditScore\"] = np.exp(-df[\"CreditScore\"] / 100.0)\n    df[\"TanhDebtRatio\"] = np.tanh(df[\"TotalDebtToIncomeRatio\"])\n    df[\"SinhAge\"] = np.sinh(df[\"Age\"] / 30.0)\n\n    df[\"CreditScore_LoanToIncome\"] = df[\"CreditScore\"] * df[\"LoanToIncome\"]\n    df[\"CreditScore_DebtRatio\"] = df[\"CreditScore\"] * df[\"TotalDebtToIncomeRatio\"]\n    df[\"CreditScore_CreditUtil\"] = df[\"CreditScore\"] * df[\"CreditCardUtilizationRate\"]\n    df[\"CreditScore_Age\"] = df[\"CreditScore\"] * df[\"Age\"]\n    df[\"CreditScore_Experience\"] = df[\"CreditScore\"] * df[\"Experience\"]\n    df[\"Age_ExperienceRatio\"] = df[\"Age\"] * df[\"ExperienceToAge\"]\n    df[\"Age_CreditHistory\"] = df[\"Age\"] * df[\"CreditHistoryToAge\"]\n    df[\"Age_DebtRatio\"] = df[\"Age\"] * df[\"TotalDebtToIncomeRatio\"]\n    df[\"MonthlyIncome_DebtService\"] = df[\"MonthlyIncome\"] * df[\"DebtServiceRatio\"]\n    df[\"DisposableIncome_CreditScore\"] = df[\"DisposableIncome\"] * df[\"CreditScore\"]\n\n    df[\"LoanToIncome_Over_Experience\"] = df[\"LoanToIncome\"] / (df[\"Experience\"] + 1)\n    df[\"CreditScore_Over_Age\"] = df[\"CreditScore\"] / (df[\"Age\"] + EPS)\n    df[\"CreditScore_Over_DebtRatio\"] = df[\"CreditScore\"] / (df[\"TotalDebtToIncomeRatio\"] + EPS)\n    df[\"Income_Over_LoanAmount\"] = df[\"MonthlyIncome\"] * 12 / (df[\"LoanAmount\"] + EPS)\n    df[\"AssetCoverage_Over_DebtRatio\"] = df[\"AssetCoverage\"] / (df[\"TotalDebtToIncomeRatio\"] + EPS)\n\n    df[\"CreditScore_Age_Income\"] = (df[\"CreditScore\"] * df[\"Age\"]) / (df[\"AnnualIncome\"] + EPS)\n    df[\"DebtRatio_Experience_Age\"] = df[\"TotalDebtToIncomeRatio\"] * df[\"Experience\"] / (df[\"Age\"] + EPS)\n    df[\"Utilization_DebtService_Income\"] = (df[\"CreditCardUtilizationRate\"] * df[\"DebtServiceRatio\"]) / (\n            df[\"MonthlyIncome\"] + EPS)\n\n    df[\"GoodCreditScore\"] = (df[\"CreditScore\"] >= 700).astype(float)\n    df[\"ExcellentCreditScore\"] = (df[\"CreditScore\"] >= 750).astype(float)\n    df[\"PoorCreditScore\"] = (df[\"CreditScore\"] < 620).astype(float)\n    df[\"HighDebtRatio\"] = (df[\"TotalDebtToIncomeRatio\"] > 0.4).astype(float)\n    df[\"LowDebtRatio\"] = (df[\"TotalDebtToIncomeRatio\"] < 0.2).astype(float)\n    df[\"HighUtilization\"] = (df[\"CreditCardUtilizationRate\"] > 0.7).astype(float)\n    df[\"LowUtilization\"] = (df[\"CreditCardUtilizationRate\"] < 0.3).astype(float)\n    df[\"YoungAge\"] = (df[\"Age\"] < 30).astype(float)\n    df[\"SeniorAge\"] = (df[\"Age\"] >= 60).astype(float)\n    df[\"WorkingAge\"] = ((df[\"Age\"] >= 25) & (df[\"Age\"] < 60)).astype(float)\n    df[\"NewWorker\"] = (df[\"Experience\"] < 2).astype(float)\n    df[\"ExperiencedWorker\"] = (df[\"Experience\"] >= 10).astype(float)\n    df[\"NoDependent\"] = (df[\"NumberOfDependents\"] == 0).astype(float)\n    df[\"ManyDependents\"] = (df[\"NumberOfDependents\"] >= 3).astype(float)\n\n    df[\"GoodFinancialHealth\"] = (\n            ((df[\"CreditScore\"] >= 700).astype(float)) * ((df[\"TotalDebtToIncomeRatio\"] < 0.4).astype(float)) * (\n        (df[\"CreditCardUtilizationRate\"] < 0.5).astype(float)))\n    df[\"RiskyProfile\"] = (\n            ((df[\"CreditScore\"] < 620).astype(float)) + ((df[\"TotalDebtToIncomeRatio\"] > 0.5).astype(float)) + (\n        (df[\"CreditCardUtilizationRate\"] > 0.8).astype(float)))\n\n    df[\"CreditScore_Normalized\"] = (df[\"CreditScore\"] - df[\"CreditScore\"].min()) / (\n            df[\"CreditScore\"].max() - df[\"CreditScore\"].min() + EPS)\n    df[\"Age_Normalized\"] = (df[\"Age\"] - df[\"Age\"].min()) / (df[\"Age\"].max() - df[\"Age\"].min() + EPS)\n    df[\"Income_Normalized\"] = (df[\"AnnualIncome\"] - df[\"AnnualIncome\"].min()) / (\n            df[\"AnnualIncome\"].max() - df[\"AnnualIncome\"].min() + EPS)\n\n    df[\"MonthlyToAnnualIncome\"] = df[\"MonthlyIncome\"] * 12 / (df[\"AnnualIncome\"] + EPS)\n    df[\"LoanDurationMonths_ToAge\"] = df[\"LoanDuration\"] / (df[\"Age\"] + EPS)\n    df[\"NetWorth_To_AnnualIncome\"] = df[\"NetWorth\"] / (df[\"AnnualIncome\"] + EPS)\n    df[\"InverseDebtRatio\"] = 1 / (df[\"TotalDebtToIncomeRatio\"] + EPS)\n    df[\"InverseLoanToIncome\"] = 1 / (df[\"LoanToIncome\"] + EPS)\n    df[\"InverseUtilization\"] = 1 / (df[\"CreditCardUtilizationRate\"] + 0.1)\n\n    add_bins(df, \"CreditScore\", [300, 500, 580, 620, 650, 680, 700, 720, 740, 760, 800, 900])\n    add_bins(df, \"AnnualIncome\", [0, 30000, 50000, 75000, 100000, 125000, 150000, 200000, 300000, np.inf])\n    add_bins(df, \"TotalDebtToIncomeRatio\", [0.0, 0.15, 0.25, 0.35, 0.45, 0.55, 0.7, 0.85, 1.0, np.inf])\n    add_bins(df, \"InterestRate\", [0.0, 0.05, 0.08, 0.12, 0.16, 0.20, 0.25, np.inf])\n    add_bins(df, \"Age\", [18, 25, 30, 35, 40, 45, 50, 55, 60, 65, 100])\n    add_bins(df, \"LoanAmount\", [0, 20000, 40000, 60000, 100000, 150000, np.inf])\n    add_bins(df, \"MonthlyIncome\", [0, 2000, 3500, 5000, 7000, 10000, 15000, 20000, np.inf])\n    add_bins(df, \"CreditCardUtilizationRate\", [0.0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1.0])\n    add_bins(df, \"Experience\", [0, 1, 2, 5, 10, 15, 20, 50])\n\n\ndef min_max_normalize(X, min_val=None, max_val=None):\n    X = np.array(X, dtype=np.float64)\n    if min_val is None or max_val is None:\n        min_val = np.min(X, axis=0)\n        max_val = np.max(X, axis=0)\n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1\n    return (X - min_val) / range_val, min_val, max_val\n\n\ndef z_score_normalize(X, mean_val=None, std_val=None):\n    X = np.array(X, dtype=np.float64)\n    if mean_val is None or std_val is None:\n        mean_val = np.nanmean(X, axis=0)\n        std_val = np.nanstd(X, axis=0)\n    std_val[std_val < EPS] = 1.0\n    result = (X - mean_val) / std_val\n    result[np.isnan(result)] = 0\n    result[np.isinf(result)] = 0\n    return result, mean_val, std_val\n\n\ndef verify_metrics(y_val, y_pred):\n\n\n    my_mse_val = my_mse(y_val, y_pred)\n    sklearn_mse_val = mean_squared_error(y_val, y_pred)\n    print(f\"\\nMSE\")\n    print(f\"My implementation:   {my_mse_val:.8f}\")\n    print(f\"sklearn:             {sklearn_mse_val:.8f}\")\n    print(f\"Match: {np.isclose(my_mse_val, sklearn_mse_val)}\")\n\n    my_mae_val = my_mae(y_val, y_pred)\n    sklearn_mae_val = mean_absolute_error(y_val, y_pred)\n    print(f\"\\nMAE\")\n    print(f\"My implementation:   {my_mae_val:.8f}\")\n    print(f\"sklearn:             {sklearn_mae_val:.8f}\")\n    print(f\"Match: {np.isclose(my_mae_val, sklearn_mae_val)}\")\n\n    my_r2_val = my_r2(y_val, y_pred)\n    sklearn_r2_val = r2_score(y_val, y_pred)\n    print(f\"\\nR²\")\n    print(f\"My implementation:   {my_r2_val:.8f}\")\n    print(f\"sklearn:             {sklearn_r2_val:.8f}\")\n    print(f\"Match: {np.isclose(my_r2_val, sklearn_r2_val)}\")\n\n    my_mape_val = my_mape(y_val, y_pred)\n    sklearn_mape_val = mean_absolute_percentage_error(y_val, y_pred) * 100\n    print(f\"\\nMAPE\")\n    print(f\"My implementation:   {my_mape_val:.8f}%\")\n    print(f\"sklearn:             {sklearn_mape_val:.8f}%\")\n    print(f\"Close match: {np.isclose(my_mape_val, sklearn_mape_val, rtol=1e-5)}\")\n\n\ndef perform_eda(train_data):\n\n\n    #print(\"\\nRiskScore Distribution:\")\n    #print(f\" {train_data['RiskScore'].mean():.2f}, Std: {train_data['RiskScore'].std():.2f}, Min: {train_data['RiskScore'].min():.2f}, Max: {train_data['RiskScore'].max():.2f}\")\n\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1, 2, 1)\n    plt.hist(train_data['RiskScore'], bins=50, edgecolor='black', alpha=0.7)\n    plt.title('RiskScore Distribution')\n    plt.xlabel('RiskScore')\n    plt.ylabel('Count')\n\n    plt.subplot(1, 2, 2)\n    plt.boxplot(train_data['RiskScore'], vert=True)\n    plt.title('RiskScore Box Plot')\n    plt.ylabel('RiskScore')\n\n    plt.tight_layout()\n    plt.savefig('01_eda_risk_score_distribution.png', dpi=100, bbox_inches='tight')\n    plt.close()\n\n\n    key_features = ['CreditScore', 'TotalDebtToIncomeRatio', 'Age',\n                    'AnnualIncome', 'CreditCardUtilizationRate']\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n    axes = axes.flatten()\n\n    for idx, feature in enumerate(key_features):\n        if feature in train_data.columns:\n            axes[idx].scatter(train_data[feature], train_data['RiskScore'],\n                              alpha=0.3, s=10, color='steelblue')\n            axes[idx].set_xlabel(feature)\n            axes[idx].set_ylabel('RiskScore')\n            axes[idx].set_title(f'{feature} vs RiskScore')\n\n            corr = train_data[[feature, 'RiskScore']].corr().iloc[0, 1]\n            axes[idx].text(0.05, 0.95, f'r={corr:.3f}',\n                           transform=axes[idx].transAxes,\n                           verticalalignment='top',\n                           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n    fig.delaxes(axes[5])\n\n    plt.tight_layout()\n    plt.savefig('02_eda_feature_dependencies.png', dpi=100, bbox_inches='tight')\n    plt.close()\n    numeric_cols = train_data.select_dtypes(include=[np.number]).columns.tolist()\n\n    top_features = ['RiskScore', 'CreditScore', 'TotalDebtToIncomeRatio',\n                    'Age', 'AnnualIncome', 'CreditCardUtilizationRate',\n                    'MonthlyIncome', 'LoanAmount', 'LoanDuration',\n                    'NumberOfOpenCreditLines', 'LengthOfCreditHistory']\n\n    available_features = [f for f in top_features if f in numeric_cols]\n\n    corr_matrix = train_data[available_features].corr()\n\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n    plt.title('Correlation Matrix')\n    plt.tight_layout()\n    plt.savefig('03_eda_correlation_matrix.png', dpi=100, bbox_inches='tight')\n    plt.close()\n\n    print(\"\\nPositive correlations (with RiskScore):\")\n    risk_corr = train_data[numeric_cols].corr()['RiskScore'].sort_values(ascending=False)\n    for feature, corr_val in risk_corr.head(10).items():\n        print(f\"{feature:<35} {corr_val:>10.6f}\")\n\ndef main():\n    train = pd.read_csv(\"train.csv\")\n    test = pd.read_csv(\"test.csv\")\n    test_ids = test['ID'].copy() if 'ID' in test.columns else pd.RangeIndex(start=0, stop=len(test))\n\n    train = train.dropna(subset=['RiskScore'])\n    train = train[train['RiskScore'].abs() <= 200].reset_index(drop=True)\n    train['RiskScore'] = train['RiskScore'].clip(0.0, 100.0)\n    perform_eda(train)\n\n    augment_features(train)\n    augment_features(test)\n    extract_date_parts(train, \"ApplicationDate\")\n    extract_date_parts(test, \"ApplicationDate\")\n\n    education_mapping = {'High School': 1, 'high school': 1, 'Associate': 2, 'associate': 2, 'Bachelor': 3,\n                         'bachelor': 3, 'Master': 4, 'master': 4, 'Doctorate': 5, 'PhD': 5, 'phd': 5, 'doctorate': 5}\n    if 'EducationLevel' in train.columns:\n        train['EducationLevel'] = train['EducationLevel'].map(education_mapping).fillna(0).astype(float)\n    if 'EducationLevel' in test.columns:\n        test['EducationLevel'] = test['EducationLevel'].map(education_mapping).fillna(0).astype(float)\n\n    X = train.drop(columns=['RiskScore'])\n    if 'ID' in X.columns:\n        X = X.drop('ID', axis=1)\n    y = train['RiskScore']\n    X_test = test.drop(columns=['ID']) if 'ID' in test.columns else test.copy()\n\n    numeric_cols = [col for col in X.columns if np.issubdtype(X[col].dtype, np.number)]\n    cat_cols = [col for col in X.columns if col not in numeric_cols]\n\n    numeric_imputer = SimpleImputer(strategy='median')\n    categorical_imputer = SimpleImputer(strategy='most_frequent')\n    X_numeric = numeric_imputer.fit_transform(X[numeric_cols])\n    X_categorical = categorical_imputer.fit_transform(X[cat_cols])\n    X_test_numeric = numeric_imputer.transform(X_test[numeric_cols])\n    X_test_categorical = categorical_imputer.transform(X_test[cat_cols])\n\n    X_numeric = signed_log1p(X_numeric)\n    X_test_numeric = signed_log1p(X_test_numeric)\n\n    ohe_kwargs = {\"handle_unknown\": \"ignore\"}\n    if \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n        ohe_kwargs[\"sparse_output\"] = False\n    else:\n        ohe_kwargs[\"sparse\"] = False\n    ohe = OneHotEncoder(**ohe_kwargs)\n    X_cat_encoded = ohe.fit_transform(X_categorical)\n    X_test_cat_encoded = ohe.transform(X_test_categorical)\n    X_processed = np.hstack([X_numeric, X_cat_encoded])\n    X_test_processed = np.hstack([X_test_numeric, X_test_cat_encoded])\n    #print(f\"Shape: {X_processed.shape[0]} × {X_processed.shape[1]}\")\n\n    X_norm, X_min, X_max = min_max_normalize(X_processed)\n    X_test_norm, _, _ = min_max_normalize(X_test_processed, X_min, X_max)\n    X_zscore, z_mean, z_std = z_score_normalize(X_processed)\n    X_test_zscore, _, _ = z_score_normalize(X_test_processed, z_mean, z_std)\n\n    X_train, X_val, y_train, y_val = train_test_split(X_norm, y, test_size=0.2, random_state=42)\n\n\n    results = {}\n\n    #print(\"\\nMethod 1: ANALYTICAL\")\n    model = MyLinearRegression(method='analytical')\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    mse = my_mse(y_val, y_pred)\n    mae = my_mae(y_val, y_pred)\n    rmse = my_rmse(y_val, y_pred)\n    r2 = my_r2(y_val, y_pred)\n    mape = my_mape(y_val, y_pred)\n    #print(f\"MSE: {mse:.6f}, RMSE: {rmse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}, MAPE: {mape:.2f}%\")\n\n    sklearn_model = LinearRegression()\n    sklearn_model.fit(X_train, y_train)\n    y_pred_sklearn = sklearn_model.predict(X_val)\n    sklearn_mse = mean_squared_error(y_val, y_pred_sklearn)\n    #print(f\"My MSE: {mse:.6f} vs sklearn: {sklearn_mse:.6f}\")\n    results['analytical'] = {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2, 'mape': mape}\n\n    #print(\"\\nMethod 2: GRADIENT DESCENT\")\n    model_gd = MyLinearRegression(method='gd', lr=0.01, epochs=500)\n    model_gd.fit(X_train, y_train)\n    y_pred_gd = model_gd.predict(X_val)\n    mse_gd = my_mse(y_val, y_pred_gd)\n    mae_gd = my_mae(y_val, y_pred_gd)\n    rmse_gd = my_rmse(y_val, y_pred_gd)\n    r2_gd = my_r2(y_val, y_pred_gd)\n    mape_gd = my_mape(y_val, y_pred_gd)\n    #print(f\"MSE: {mse_gd:.6f}, RMSE: {rmse_gd:.6f}, MAE: {mae_gd:.6f}, R²: {r2_gd:.6f}, MAPE: {mape_gd:.2f}%\")\n    results['gd'] = {'mse': mse_gd, 'rmse': rmse_gd, 'mae': mae_gd, 'r2': r2_gd, 'mape': mape_gd}\n\n    #print(\"\\nMethod 3: STOCHASTIC GRADIENT DESCENT\")\n    model_sgd = MyLinearRegression(method='sgd', lr=0.01, epochs=500, batch_size=32)\n    model_sgd.fit(X_train, y_train)\n    y_pred_sgd = model_sgd.predict(X_val)\n    mse_sgd = my_mse(y_val, y_pred_sgd)\n    mae_sgd = my_mae(y_val, y_pred_sgd)\n    rmse_sgd = my_rmse(y_val, y_pred_sgd)\n    r2_sgd = my_r2(y_val, y_pred_sgd)\n    mape_sgd = my_mape(y_val, y_pred_sgd)\n    #print(f\"MSE: {mse_sgd:.6f}, RMSE: {rmse_sgd:.6f}, MAE: {mae_sgd:.6f}, R²: {r2_sgd:.6f}, MAPE: {mape_sgd:.2f}%\")\n    results['sgd'] = {'mse': mse_sgd, 'rmse': rmse_sgd, 'mae': mae_sgd, 'r2': r2_sgd, 'mape': mape_sgd}\n\n    verify_metrics(y_val, y_pred)\n\n\n    #print(\"K-FOLD CROSS-VALIDATION\")\n\n    kf_mse = k_fold_cross_validation(X_norm, y, k=5, method='analytical')\n    kf_gd = k_fold_cross_validation(X_norm, y, k=5, method='gd')\n\n    print(f\"\\nsklearn Cross-validated MSE: {np.mean(kf_mse):.4f} ± {np.std(kf_mse):.4f}\")\n    print(f\"analytical Cross-validated MSE: {np.mean(kf_mse):.4f} ± {np.std(kf_mse):.4f}\")\n    print(f\"gd Cross-validated MSE: {np.mean(kf_gd):.4f} ± {np.std(kf_gd):.4f}\")\n\n    loo_mse, loo_mae = leave_one_out_cross_validation(X_norm, y, method='analytical', max_samples=500)\n    print(f\"\\nLOO Cross-validated MSE: {np.mean(loo_mse):.4f} ± {np.std(loo_mse):.4f}\")\n\n    model_ens1 = MyLinearRegression(method='analytical')\n    model_ens1.fit(X_norm, y)\n\n    model_ens2 = MyLinearRegression(method='gd', lr=0.01, epochs=500)\n    model_ens2.fit(X_norm, y)\n\n    model_ens3 = MyLinearRegression(method='sgd', lr=0.01, epochs=500, batch_size=32)\n    model_ens3.fit(X_norm, y)\n\n    pred1 = model_ens1.predict(X_test_norm)\n    pred2 = model_ens2.predict(X_test_norm)\n    pred3 = model_ens3.predict(X_test_norm)\n\n    test_predictions = 0.5 * pred1 + 0.25 * pred2 + 0.25 * pred3\n    test_predictions = np.clip(test_predictions, 0.0, 100.0)\n\n    submission = pd.DataFrame({'ID': test_ids, 'RiskScore': test_predictions})\n    submission.to_csv('submission.csv', index=False)\n\n    print(\n        f\"\\nSubmission: min={test_predictions.min():.2f}, max={test_predictions.max():.2f}, mean={test_predictions.mean():.2f}\")\n\n    y_pred_final = model_ens1.predict(X_val)\n    final_mse = my_mse(y_val, y_pred_final)\n    print(f\"Result: MSE = {final_mse:.2f}\")\n\nif __name__ == \"__main__\":\n    main()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}