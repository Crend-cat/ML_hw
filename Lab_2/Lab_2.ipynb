{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "e75fe8f5-e6f3-4617-b4e9-2b5333ba0889",
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import clone\n\nfrom sklearn.metrics import (\n    roc_auc_score, roc_curve,\n    precision_score, recall_score, f1_score,\n    accuracy_score,\n    precision_recall_curve, auc\n)\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nimport optuna\n\nEPS = 1e-3\n\ndef signed_log1p(data):\n    return np.sign(data) * np.log1p(np.abs(data))\n\n\ndef add_bins(df, column, bins, fmt=\"num\"):\n    if column not in df.columns:\n        return\n    labels = []\n    for i in range(len(bins) - 1):\n        lo, hi = bins[i], bins[i + 1]\n        if np.isinf(hi):\n            labels.append(f\"{lo}{'+' if fmt == 'num' else ''}\")\n        else:\n            labels.append(f\"{lo}-{hi}\")\n    df[f\"{column}Band\"] = (\n        pd.cut(df[column], bins=bins, labels=labels, include_lowest=True)\n        .astype(str)\n    )\n\n\ndef augment_features(df):\n    df[\"InterestRateSpread\"] = df[\"InterestRate\"] - df[\"BaseInterestRate\"]\n    df[\"LoanToIncome\"] = df[\"LoanAmount\"] / (df[\"AnnualIncome\"] + EPS)\n\n    total_debt = df[\"MonthlyLoanPayment\"] + df[\"MonthlyDebtPayments\"]\n    df[\"DebtServiceRatio\"] = total_debt / (df[\"MonthlyIncome\"] + EPS)\n    df[\"DisposableIncome\"] = df[\"MonthlyIncome\"] - total_debt\n    df[\"AssetCoverage\"] = df[\"TotalAssets\"] / (df[\"TotalLiabilities\"] + EPS)\n\n    df[\"LiabilityGap\"] = df[\"TotalLiabilities\"] - df[\"TotalAssets\"]\n    df[\"SignedLogLiabilityGap\"] = signed_log1p(df[\"LiabilityGap\"])\n    df.drop(columns=[\"LiabilityGap\"], inplace=True)\n\n    df[\"NetWorthToLiabilities\"] = df[\"NetWorth\"] / (df[\"TotalLiabilities\"] + EPS)\n    df[\"NetWorthToIncome\"] = df[\"NetWorth\"] / (df[\"AnnualIncome\"] + EPS)\n    df[\"UtilizationPerLine\"] = df[\"CreditCardUtilizationRate\"] / (df[\"NumberOfOpenCreditLines\"] + 1)\n    df[\"InquiryPerLine\"] = df[\"NumberOfCreditInquiries\"] / (df[\"NumberOfOpenCreditLines\"] + 1)\n    df[\"IncomePerDependent\"] = df[\"AnnualIncome\"] / (df[\"NumberOfDependents\"] + 1)\n    df[\"ExperienceToAge\"] = df[\"Experience\"] / (df[\"Age\"] + EPS)\n    df[\"LoanDurationYears\"] = df[\"LoanDuration\"] / 12.0\n    df[\"CreditHistoryToAge\"] = df[\"LengthOfCreditHistory\"] / (df[\"Age\"] + EPS)\n    df[\"IncomeDiscrepancy\"] = df[\"AnnualIncome\"] - (df[\"MonthlyIncome\"] * 12.0)\n    df[\"AgeAfterExperience\"] = df[\"Age\"] - df[\"Experience\"]\n\n    parsed = pd.to_datetime(df[\"ApplicationDate\"], errors=\"coerce\")\n    df[\"ApplicationDateWeek\"] = parsed.dt.isocalendar().week.astype(float)\n    df[\"ApplicationDateDayOfYear\"] = parsed.dt.dayofyear\n    df[\"ApplicationDateQuarter\"] = parsed.dt.quarter\n\n    df[\"CreditScore_2\"] = df[\"CreditScore\"] ** 2\n    df[\"Age_2\"] = df[\"Age\"] ** 2\n    df[\"SqrtAnnualIncome\"] = np.sqrt(np.abs(df[\"AnnualIncome\"]) + EPS)\n    df[\"SqrtLoanAmount\"] = np.sqrt(np.abs(df[\"LoanAmount\"]) + EPS)\n    df[\"SqrtMonthlyIncome\"] = np.sqrt(np.abs(df[\"MonthlyIncome\"]) + EPS)\n    df[\"LogCreditScore\"] = np.log1p(df[\"CreditScore\"])\n    df[\"LogExperience\"] = np.log1p(df[\"Experience\"])\n    df[\"LogAge\"] = np.log1p(df[\"Age\"])\n    df[\"LogAnnualIncome\"] = np.log1p(df[\"AnnualIncome\"] + EPS)\n\n    df[\"CreditScore_LoanToIncome\"] = df[\"CreditScore\"] * df[\"LoanToIncome\"]\n    df[\"CreditScore_DebtRatio\"] = df[\"CreditScore\"] * df[\"TotalDebtToIncomeRatio\"]\n    df[\"Age_DebtRatio\"] = df[\"Age\"] * df[\"TotalDebtToIncomeRatio\"]\n\n    df[\"GoodCreditScore\"] = (df[\"CreditScore\"] >= 700).astype(float)\n    df[\"ExcellentCreditScore\"] = (df[\"CreditScore\"] >= 750).astype(float)\n    df[\"HighDebtRatio\"] = (df[\"TotalDebtToIncomeRatio\"] > 0.4).astype(float)\n    df[\"HighUtilization\"] = (df[\"CreditCardUtilizationRate\"] > 0.7).astype(float)\n    df[\"YoungAge\"] = (df[\"Age\"] < 30).astype(float)\n    df[\"ExperiencedWorker\"] = (df[\"Experience\"] >= 10).astype(float)\n\n    add_bins(df, \"CreditScore\", [300, 500, 620, 700, 750, 800, 900])\n    add_bins(df, \"AnnualIncome\", [0, 30000, 75000, 125000, 200000, np.inf])\n    add_bins(df, \"TotalDebtToIncomeRatio\", [0.0, 0.25, 0.4, 0.7, np.inf])\n\n\ndef my_accuracy(y_true, y_pred):\n    return (y_true == y_pred).mean()\n\n\ndef my_precision(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    return tp / (tp + fp + 1e-12)\n\n\ndef my_recall(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    return tp / (tp + fn + 1e-12)\n\n\ndef my_f1(y_true, y_pred):\n    p = my_precision(y_true, y_pred)\n    r = my_recall(y_true, y_pred)\n    return 2 * p * r / (p + r + 1e-12)\n\n\ndef my_roc_auc(y_true, y_score):\n    return roc_auc_score(y_true, y_score)\n\n\ndef my_pr_auc(y_true, y_score):\n    precision, recall, _ = precision_recall_curve(y_true, y_score)\n    return auc(recall, precision)\n\n\nclass MyBagging:\n    def __init__(self, base_estimator, n_estimators=10, max_samples=0.8, random_state=42):\n        self.base_estimator = base_estimator\n        self.n_estimators = n_estimators\n        self.max_samples = max_samples\n        self.random_state = random_state\n        self.models_ = []\n\n    def fit(self, X, y):\n        X = np.asarray(X)\n        y = np.asarray(y)\n\n        rng = np.random.RandomState(self.random_state)\n        n_samples = X.shape[0]\n        self.models_ = []\n\n        for i in range(self.n_estimators):\n            idx = rng.choice(n_samples, int(self.max_samples * n_samples), replace=True)\n            model = clone(self.base_estimator)\n            model.fit(X[idx], y[idx])\n            self.models_.append(model)\n        return self\n\n\n    def predict_proba(self, X):\n        probs = np.array([m.predict_proba(X)[:, 1] for m in self.models_])\n        proba_mean = probs.mean(axis=0)\n        return np.vstack([1 - proba_mean, proba_mean]).T\n\n    def predict(self, X, threshold=0.5):\n        return (self.predict_proba(X)[:, 1] >= threshold).astype(int)\n\nclass MyGradientBoosting:\n    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.trees_ = []\n        self.init_score_ = None\n\n    def fit(self, X, y):\n        y = y.astype(float)\n        p = np.clip(y.mean(), 1e-6, 1 - 1e-6)\n        self.init_score_ = np.log(p / (1 - p))  # logit\n        F = np.full(len(y), self.init_score_)\n        self.trees_ = []\n\n        for _ in range(self.n_estimators):\n            p = 1 / (1 + np.exp(-F))\n            residual = y - p\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, residual)\n            self.trees_.append(tree)\n            F += self.learning_rate * tree.predict(X)\n\n        return self\n\n    def _score(self, X):\n        F = np.full(X.shape[0], self.init_score_)\n        for tree in self.trees_:\n            F += self.learning_rate * tree.predict(X)\n        return 1 / (1 + np.exp(-F))\n\n    def predict_proba(self, X):\n        proba1 = self._score(X)\n        return np.vstack([1 - proba1, proba1]).T\n\n    def predict(self, X, threshold=0.5):\n        return (self._score(X) >= threshold).astype(int)\n\ndef main():\n    \n    train_raw = pd.read_csv(\"train_c.csv\")\n\n    # EDA: таргет\n    plt.figure(figsize=(4, 3))\n    train_raw[\"LoanApproved\"].value_counts(normalize=True).plot(kind=\"bar\")\n    plt.title(\"Распределение LoanApproved\")\n    plt.tight_layout()\n    plt.savefig(\"eda_target_distribution.png\")\n\n    # EDA: пара зависимостей\n    plt.figure(figsize=(5, 4))\n    plt.scatter(train_raw[\"CreditScore\"], train_raw[\"LoanAmount\"], alpha=0.3)\n    plt.xlabel(\"CreditScore\")\n    plt.ylabel(\"LoanAmount\")\n    plt.title(\"CreditScore vs LoanAmount\")\n    plt.tight_layout()\n    plt.savefig(\"eda_credit_vs_loan.png\")\n\n    # EDA: корреляционная матрица\n    plt.figure(figsize=(10, 8))\n    corr = train_raw.corr(numeric_only=True)\n    sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n    plt.title(\"Correlation matrix\")\n    plt.tight_layout()\n    plt.savefig(\"eda_corr_matrix.png\")\n\n\n    train = train_raw.copy()\n    test = pd.read_csv(\"test_c.csv\")\n    test_ids = test[\"ID\"].copy()\n\n\n    print(f\"\\nПропущенные значения в LoanApproved: {train['LoanApproved'].isnull().sum()}\")\n    print(f\"Распределение до обработки: {train['LoanApproved'].value_counts().to_dict()}\")\n\n    train = train.dropna(subset=[\"LoanApproved\"]).reset_index(drop=True)\n    y = train[\"LoanApproved\"].astype(int)\n\n    print(f\"\\nПосле удаления NaN: {len(train)} строк\")\n    print(f\"Баланс классов: {y.value_counts(normalize=True).to_dict()}\")\n\n    augment_features(train)\n    augment_features(test)\n\n    education_mapping = {\n        \"High School\": 1, \"high school\": 1,\n        \"Associate\": 2, \"associate\": 2,\n        \"Bachelor\": 3, \"bachelor\": 3,\n        \"Master\": 4, \"master\": 4,\n        \"Doctorate\": 5, \"doctorate\": 5,\n        \"PhD\": 5, \"phd\": 5\n    }\n\n    if \"EducationLevel\" in train.columns:\n        train[\"EducationLevel\"] = train[\"EducationLevel\"].map(education_mapping).fillna(0).astype(float)\n    if \"EducationLevel\" in test.columns:\n        test[\"EducationLevel\"] = test[\"EducationLevel\"].map(education_mapping).fillna(0).astype(float)\n\n    # 4. Разделение признаков / таргета\n    X = train.drop(columns=[\"LoanApproved\"])\n    X_test = test.drop(columns=[\"ID\"])\n\n    # синхронизация признаков\n    common_cols = list(set(X.columns) & set(X_test.columns))\n    X = X[common_cols]\n    X_test = X_test[common_cols]\n\n    print(f\"\\nПосле синхронизации: {len(common_cols)} признаков\")\n    print(f\"X shape: {X.shape}, X_test shape: {X_test.shape}\")\n\n    # Числовые / категориальные\n    numeric_cols = [c for c in X.columns if np.issubdtype(X[c].dtype, np.number)]\n    cat_cols = [c for c in X.columns if c not in numeric_cols]\n    print(f\"Признаки: {len(numeric_cols)} числовых, {len(cat_cols)} категориальных\")\n\n    # Пропуски\n    numeric_imputer = SimpleImputer(strategy=\"median\")\n    categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n\n    X_numeric = numeric_imputer.fit_transform(X[numeric_cols])\n    X_categorical = categorical_imputer.fit_transform(X[cat_cols])\n    X_test_numeric = numeric_imputer.transform(X_test[numeric_cols])\n    X_test_categorical = categorical_imputer.transform(X_test[cat_cols])\n\n    # лог‑преобразование\n    X_numeric = signed_log1p(X_numeric)\n    X_test_numeric = signed_log1p(X_test_numeric)\n\n    # OHE\n    if len(cat_cols) > 0:\n        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, dtype=\"float32\")\n        X_cat_encoded = ohe.fit_transform(X_categorical)\n        X_test_cat_encoded = ohe.transform(X_test_categorical)\n        X_processed = np.hstack([X_numeric, X_cat_encoded])\n        X_test_processed = np.hstack([X_test_numeric, X_test_cat_encoded])\n    else:\n        X_processed = X_numeric\n        X_test_processed = X_test_numeric\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_processed)\n    X_test_scaled = scaler.transform(X_test_processed)\n\n    print(f\"Финальная размерность: {X_scaled.shape}\")\n\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n    )\n\n    y_train = np.asarray(y_train)\n    y_val = np.asarray(y_val)\n\n\n    print(\"\\nПроверка собственных метрик\")\n    tmp_clf = GradientBoostingClassifier(random_state=42)\n    tmp_clf.fit(X_train, y_train)\n    y_tmp = tmp_clf.predict(X_val)\n\n    print(\"Accuracy:\", my_accuracy(y_val, y_tmp), accuracy_score(y_val, y_tmp))\n    print(\"Precision:\", my_precision(y_val, y_tmp), precision_score(y_val, y_tmp))\n    print(\"Recall:\", my_recall(y_val, y_tmp), recall_score(y_val, y_tmp))\n    print(\"F1:\", my_f1(y_val, y_tmp), f1_score(y_val, y_tmp))\n\n\n    print(\"\\nСравнение бэггинга\")\n    base = DecisionTreeClassifier(max_depth=5, random_state=42)\n\n    my_bag = MyBagging(base_estimator=base, n_estimators=20, max_samples=0.8, random_state=42)\n    my_bag.fit(X_train, y_train)\n    my_bag_proba = my_bag.predict_proba(X_val)[:, 1]\n    my_bag_auc = roc_auc_score(y_val, my_bag_proba)\n\n    sk_bag = BaggingClassifier(estimator=base, n_estimators=20, max_samples=0.8, random_state=42)\n    sk_bag.fit(X_train, y_train)\n    sk_bag_proba = sk_bag.predict_proba(X_val)[:, 1]\n    sk_bag_auc = roc_auc_score(y_val, sk_bag_proba)\n\n    print(f\"MyBagging ROC-AUC: {my_bag_auc:.4f}\")\n    print(f\"sklearn Bagging ROC-AUC: {sk_bag_auc:.4f}\")\n\n\n    print(\"\\nСравнение градиентного бустинга\")\n    my_gb = MyGradientBoosting(n_estimators=5, learning_rate=0.1, max_depth=1)\n    my_gb.fit(X_train, y_train)\n    my_gb_proba = my_gb.predict_proba(X_val)[:, 1]\n    my_gb_auc = roc_auc_score(y_val, my_gb_proba)\n\n    sk_gb = GradientBoostingClassifier(\n        n_estimators=50,\n        learning_rate=0.1,\n        max_depth=2,\n        random_state=42\n    )\n    sk_gb.fit(X_train, y_train)\n    sk_gb_proba = sk_gb.predict_proba(X_val)[:, 1]\n    sk_gb_auc = roc_auc_score(y_val, sk_gb_proba)\n\n    print(f\"MyGradientBoosting ROC-AUC: {my_gb_auc:.4f}\")\n    print(f\"sklearn GradientBoosting ROC-AUC: {sk_gb_auc:.4f}\")\n\n\n    print(\"\\nСравнение бустингов (sklearn / LGBM / XGB / CatBoost)\")\n\n    models = {\n        \"sklearn_GB\": GradientBoostingClassifier(\n            n_estimators=30,\n            learning_rate=0.1,\n            max_depth=2,\n            random_state=42\n        ),\n        \"LightGBM\": lgb.LGBMClassifier(\n            n_estimators=50,\n            learning_rate=0.1,\n            num_leaves=31,\n            max_depth=-1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42\n        ),\n        \"XGBoost\": xgb.XGBClassifier(\n            n_estimators=50,\n            learning_rate=0.1,\n            max_depth=4,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            eval_metric=\"logloss\",\n            random_state=42\n        ),\n        \"CatBoost\": CatBoostClassifier(\n            iterations=50,\n            learning_rate=0.1,\n            depth=4,\n            loss_function=\"Logloss\",\n            eval_metric=\"AUC\",\n            verbose=False,\n            random_state=42\n        )\n    }\n\n    model_scores = {}\n\n    subsample_cmp = min(3000, X_train.shape[0])\n    idx_cmp = np.random.choice(X_train.shape[0], subsample_cmp, replace=False)\n    X_cmp = X_train[idx_cmp]\n    y_cmp = y_train[idx_cmp]\n\n    for name, mdl in models.items():\n        mdl.fit(X_cmp, y_cmp)  # вместо X_train, y_train\n        proba = mdl.predict_proba(X_val)[:, 1]\n        score = roc_auc_score(y_val, proba)\n        print(f\"{name}: ROC-AUC = {score:.4f}\")\n        model_scores[name] = score\n\n    best_model_name = max(model_scores, key=model_scores.get)\n    print(f\"\\nЛучшая модель по ROC-AUC: {best_model_name} ({model_scores[best_model_name]:.4f})\")\n\n    def objective(trial):\n        params = {\n            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 600),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 64),\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n            \"max_depth\": trial.suggest_int(\"max_depth\", -1, 10),\n            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n            \"random_state\": 42,\n        }\n        clf = lgb.LGBMClassifier(**params)\n        clf.fit(X_train, y_train)\n        proba = clf.predict_proba(X_val)[:, 1]\n        return roc_auc_score(y_val, proba)\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=30)\n\n    print(\"Best params:\", study.best_params)\n    print(\"Best ROC-AUC:\", study.best_value)\n\n    best_lgbm = lgb.LGBMClassifier(**study.best_params)\n    best_lgbm.fit(X_train, y_train)\n    y_val_proba = best_lgbm.predict_proba(X_val)[:, 1]\n    y_val_pred = (y_val_proba >= 0.5).astype(int)\n\n    # метрики\n    val_roc_auc = roc_auc_score(y_val, y_val_proba)\n    val_precision = precision_score(y_val, y_val_pred)\n    val_recall = recall_score(y_val, y_val_pred)\n    val_f1 = f1_score(y_val, y_val_pred)\n    val_pr_auc = my_pr_auc(y_val, y_val_proba)\n\n    print(\"\\nМЕТРИКИ ЛУЧШЕЙ МОДЕЛИ validation:\")\n    print(f\"ROC-AUC: {val_roc_auc:.4f}\")\n    print(f\"Precision: {val_precision:.4f}\")\n    print(f\"Recall:    {val_recall:.4f}\")\n    print(f\"F1-Score:  {val_f1:.4f}\")\n    print(f\"PR-AUC:    {val_pr_auc:.4f}\")\n\n    cv_scores = cross_val_score(best_lgbm, X_scaled, y, cv=3, scoring=\"roc_auc\", n_jobs=1)\n    print(f\"\\nCV ROC-AUC: {cv_scores.mean():.4f} (+- {cv_scores.std():.4f})\")\n\n    y_test_proba = best_lgbm.predict_proba(X_test_scaled)[:, 1]\n    y_test_pred = (y_test_proba >= 0.5).astype(int)\n\n    submission = pd.DataFrame({\n        \"ID\": test_ids,\n        \"LoanApproved\": y_test_pred\n    })\n\n    submission.to_csv(\"submission.csv\", index=False)\n   \n    fpr, tpr, _ = roc_curve(y_val, y_val_proba)\n    pr_precision, pr_recall, _ = precision_recall_curve(y_val, y_val_proba)\n    pr_auc = auc(pr_recall, pr_precision)\n\n    plt.figure(figsize=(14, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(fpr, tpr, label=f\"ROC (AUC = {val_roc_auc:.4f})\", linewidth=2.5, color=\"blue\")\n    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\", linewidth=1)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curve\")\n    plt.legend()\n    plt.grid(alpha=0.3)\n\n    plt.subplot(1, 2, 2)\n    plt.plot(pr_recall, pr_precision, label=f\"PR (AUC = {pr_auc:.4f})\", linewidth=2.5, color=\"green\")\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(\"Precision-Recall Curve\")\n    plt.legend()\n    plt.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(\"roc_pr_curves.png\", dpi=300, bbox_inches=\"tight\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}